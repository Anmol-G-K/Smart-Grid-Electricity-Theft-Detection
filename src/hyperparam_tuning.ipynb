{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "X = df.drop(['theft', 'Class'], axis=1)\n",
    "y = df['theft']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model names\n",
    "models = ['lgbm', 'xgboost', 'logreg', 'rf']\n",
    "\n",
    "# Dictionary to store best parameters\n",
    "best_params = {}\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    if model_name == 'lgbm':\n",
    "        param = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'max_bin': trial.suggest_int('max_bin', 100, 500),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "            'class_weight': class_weight_dict\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    elif model_name == 'xgboost':\n",
    "        param = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'scale_pos_weight': class_weights[1]\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    elif model_name == 'logreg':\n",
    "        param = {\n",
    "            'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "            'C': trial.suggest_loguniform('C', 1e-5, 1e5),\n",
    "            'multi_class': 'ovr',\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "        model = LogisticRegression(**param)\n",
    "    \n",
    "    elif model_name == 'rf':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "        model = RandomForestClassifier(**param)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Optimize each model separately\n",
    "for model_name in models:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, model_name), n_trials=30)\n",
    "    best_params[model_name] = study.best_trial.params\n",
    "    print(f\"Best parameters for {model_name}: {study.best_trial.params}\")\n",
    "\n",
    "# Print all best parameters\n",
    "print(\"\\nFinal Best Parameters for Each Model:\")\n",
    "for model, params in best_params.items():\n",
    "    print(f\"{model}: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "gass_columns = [\n",
    "    'Gas:Facility [kW](Hourly)',\n",
    "    'Heating:Gas [kW](Hourly)',\n",
    "    'InteriorEquipment:Gas [kW](Hourly)',\n",
    "    'Water Heater:WaterSystems:Gas [kW](Hourly)'\n",
    "]\n",
    "\n",
    "df.drop(columns=gass_columns, inplace=True)\n",
    "\n",
    "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "X = df.drop(['theft', 'Class'], axis=1)\n",
    "y = df['theft']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model names\n",
    "models = ['lgbm', 'xgboost', 'logreg', 'rf']\n",
    "\n",
    "# Dictionary to store best parameters\n",
    "best_params = {}\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    if model_name == 'lgbm':\n",
    "        param = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'max_bin': trial.suggest_int('max_bin', 100, 500),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "            'class_weight': class_weight_dict\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    elif model_name == 'xgboost':\n",
    "        param = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'scale_pos_weight': class_weights[1]\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    elif model_name == 'logreg':\n",
    "        param = {\n",
    "            'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "            'C': trial.suggest_loguniform('C', 1e-5, 1e5),\n",
    "            'multi_class': 'ovr',\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "        model = LogisticRegression(**param)\n",
    "    \n",
    "    elif model_name == 'rf':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "        model = RandomForestClassifier(**param)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Optimize each model separately\n",
    "for model_name in models:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, model_name), n_trials=30)\n",
    "    best_params[model_name] = study.best_trial.params\n",
    "    print(f\"Best parameters for {model_name}: {study.best_trial.params}\")\n",
    "\n",
    "# Print all best parameters\n",
    "print(\"\\nFinal Best Parameters for Each Model:\")\n",
    "for model, params in best_params.items():\n",
    "    print(f\"{model}: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "X = df.drop(['theft', 'Class'], axis=1)\n",
    "y = df['theft']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Dictionary to store best parameters\n",
    "best_params = {}\n",
    "\n",
    "# Extra Trees Optimization\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'class_weight': 'balanced',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = ExtraTreesClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Store best parameters\n",
    "best_params['extra_trees'] = study.best_trial.params\n",
    "print(f\"Best parameters for Extra Trees: {study.best_trial.params}\")\n",
    "\n",
    "# Print final best parameters\n",
    "print(\"\\nFinal Best Parameters for Extra Trees:\")\n",
    "print(best_params['extra_trees'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "gass_columns = [\n",
    "    'Gas:Facility [kW](Hourly)',\n",
    "    'Heating:Gas [kW](Hourly)',\n",
    "    'InteriorEquipment:Gas [kW](Hourly)',\n",
    "    'Water Heater:WaterSystems:Gas [kW](Hourly)'\n",
    "]\n",
    "\n",
    "df.drop(columns=gass_columns, inplace=True)\n",
    "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "X = df.drop(['theft', 'Class'], axis=1)\n",
    "y = df['theft']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Dictionary to store best parameters\n",
    "best_params = {}\n",
    "\n",
    "# Extra Trees Optimization\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'class_weight': 'balanced',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = ExtraTreesClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Store best parameters\n",
    "best_params['extra_trees'] = study.best_trial.params\n",
    "print(f\"Best parameters for Extra Trees: {study.best_trial.params}\")\n",
    "\n",
    "# Print final best parameters\n",
    "print(\"\\nFinal Best Parameters for Extra Trees:\")\n",
    "print(best_params['extra_trees'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEE213-IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
