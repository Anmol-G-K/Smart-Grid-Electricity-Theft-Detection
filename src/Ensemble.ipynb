{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "\n",
    "def check_gpu_availability():\n",
    "    try:\n",
    "        # LightGBM GPU Check\n",
    "        lgbm_params = {\"device\": \"gpu\"}\n",
    "        lgb.LGBMClassifier(**lgbm_params)\n",
    "        lgbm_gpu = True\n",
    "    except Exception:\n",
    "        lgbm_gpu = False\n",
    "\n",
    "    # XGBoost GPU Check\n",
    "    xgb_gpu = torch.cuda.is_available()  # XGBoost can use CUDA if available\n",
    "\n",
    "    # PyTorch GPU Check\n",
    "    torch_gpu = torch.cuda.is_available()\n",
    "\n",
    "    gpu_status = {\n",
    "        \"LightGBM\": lgbm_gpu,\n",
    "        \"XGBoost\": xgb_gpu,\n",
    "        \"PyTorch\": torch_gpu\n",
    "    }\n",
    "\n",
    "    for lib, available in gpu_status.items():\n",
    "        print(f\" {lib} GPU Support: {'Available' if available else 'Not Available'}\")\n",
    "\n",
    "    return gpu_status\n",
    "\n",
    "# Run the check before training\n",
    "gpu_status = check_gpu_availability()\n",
    "\n",
    "# Example usage:\n",
    "if gpu_status[\"LightGBM\"]:\n",
    "    print(\"Using GPU for LightGBM training...\")\n",
    "else:\n",
    "    print(\"Falling back to CPU for LightGBM.\")\n",
    "\n",
    "if gpu_status[\"XGBoost\"]:\n",
    "    print(\"Using GPU for XGBoost training...\")\n",
    "else:\n",
    "    print(\"Falling back to CPU for XGBoost.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#  Check GPU Availability\n",
    "gpu_available = xgb.XGBClassifier().get_params().get('device', 'cpu') == 'cuda'\n",
    "print(f\" GPU Available: {gpu_available}\")\n",
    "\n",
    "#  Load dataset\n",
    "print(\"\\n Loading dataset...\")\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "print(f\" Dataset Loaded! Shape: {df.shape}\")\n",
    "\n",
    "#  Encode categorical labels\n",
    "print(\"\\n Encoding categorical labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Class\"] = label_encoder.fit_transform(df[\"Class\"])\n",
    "df[\"theft\"] = label_encoder.fit_transform(df[\"theft\"])\n",
    "print(\" Encoding complete!\")\n",
    "\n",
    "#  Feature selection\n",
    "feature_cols = [\n",
    "    \"Electricity:Facility [kW](Hourly)\", \"Fans:Electricity [kW](Hourly)\", \"Cooling:Electricity [kW](Hourly)\",\n",
    "    \"Heating:Electricity [kW](Hourly)\", \"InteriorLights:Electricity [kW](Hourly)\", \"InteriorEquipment:Electricity [kW](Hourly)\",\n",
    "    \"Gas:Facility [kW](Hourly)\", \"Heating:Gas [kW](Hourly)\", \"InteriorEquipment:Gas [kW](Hourly)\",\n",
    "    \"Water Heater:WaterSystems:Gas [kW](Hourly)\"\n",
    "]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"theft\"].values\n",
    "n_classes = len(np.unique(y))  # Number of unique classes\n",
    "\n",
    "#  Compute Class Weights for Imbalance Handling\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in np.unique(y)}\n",
    "\n",
    "#  Split dataset\n",
    "print(\"\\n Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(f\" Split complete! Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "#  Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#  Define XGBoost Model (Handles Class Weights)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=150, learning_rate=0.03, max_depth=7,\n",
    "    tree_method=\"gpu_hist\" if gpu_available else \"hist\",\n",
    "    objective=\"multi:softprob\", num_class=n_classes,\n",
    "    device=\"cuda\" if gpu_available else \"cpu\", n_jobs=-1\n",
    ")\n",
    "\n",
    "#  Train XGBoost with Sample Weights\n",
    "print(\"\\n Training XGBoost (Handling Class Imbalance)...\")\n",
    "xgb_model.fit(X_train, y_train, sample_weight=np.array([class_weight_dict[i] for i in y_train]))\n",
    "print(\" XGBoost Training Completed!\")\n",
    "\n",
    "#  Define Base Models (RF + XGB + Logistic Regression)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=150, max_depth=25, class_weight=\"balanced\", random_state=42, n_jobs=-1)),\n",
    "    ('xgb', xgb_model),\n",
    "    ('logreg', LogisticRegression(class_weight=\"balanced\", max_iter=500, solver=\"saga\", multi_class=\"multinomial\", n_jobs=-1))\n",
    "]\n",
    "\n",
    "#  Define Meta-Model (LightGBM)\n",
    "meta_model = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, max_depth=7, \n",
    "                                num_leaves=40, random_state=42, \n",
    "                                is_unbalance=True,  # Helps with class imbalance\n",
    "                                min_gain_to_split=0.01,  # Prevents over-pruning\n",
    "                                device=\"gpu\" if gpu_available else \"cpu\")\n",
    "\n",
    "#  Define Stacking Classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "#  Train Stacking Model\n",
    "print(\"\\n Training Optimized Stacking Classifier...\")\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\" Optimized Stacking Classifier Training Completed in {end_time - start_time:.2f} seconds!\")\n",
    "\n",
    "#  Predictions\n",
    "print(\"\\n Making predictions...\")\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)\n",
    "\n",
    "#  Evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1-score\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    }\n",
    "    print(f\"\\nüîç {model_name} Results:\")\n",
    "    print(results)\n",
    "    print(f\"\\n Confusion Matrix ({model_name}):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "#  Evaluate Stacked Model\n",
    "print(\"\\ Evaluating Optimized Stacking Classifier...\")\n",
    "evaluate_model(y_test, y_pred, y_pred_proba, \"Stacking Classifier\")\n",
    "\n",
    "print(\"\\n Training & Evaluation Complete! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gass columns dropped\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#  Check GPU Availability\n",
    "gpu_available = xgb.XGBClassifier().get_params().get('device', 'cpu') == 'cuda'\n",
    "print(f\" GPU Available: {gpu_available}\")\n",
    "\n",
    "#  Load dataset\n",
    "print(\"\\n Loading dataset...\")\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "print(f\" Dataset Loaded! Shape: {df.shape}\")\n",
    "\n",
    "#  Drop columns related to gas usage\n",
    "gas_columns = [\n",
    "    \"Gas:Facility [kW](Hourly)\",\n",
    "    \"Heating:Gas [kW](Hourly)\",\n",
    "    \"InteriorEquipment:Gas [kW](Hourly)\",\n",
    "    \"Water Heater:WaterSystems:Gas [kW](Hourly)\"\n",
    "]\n",
    "df = df.drop(columns=gas_columns, errors='ignore')\n",
    "\n",
    "#  Encode categorical labels\n",
    "print(\"\\n Encoding categorical labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Class\"] = label_encoder.fit_transform(df[\"Class\"])\n",
    "df[\"theft\"] = label_encoder.fit_transform(df[\"theft\"])\n",
    "print(\" Encoding complete!\")\n",
    "\n",
    "#  Separate features and target\n",
    "X = df.drop(columns=[\"theft\"])\n",
    "y = df[\"theft\"]\n",
    "n_classes = len(np.unique(y))  # Number of unique classes\n",
    "\n",
    "#  Compute Class Weights for Imbalance Handling\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in np.unique(y)}\n",
    "\n",
    "#  Split dataset\n",
    "print(\"\\n Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(f\" Split complete! Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "#  Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#  Define XGBoost Model (Handles Class Weights)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=150, learning_rate=0.03, max_depth=7,\n",
    "    tree_method=\"gpu_hist\" if gpu_available else \"hist\",\n",
    "    objective=\"multi:softprob\", num_class=n_classes,\n",
    "    device=\"cuda\" if gpu_available else \"cpu\", n_jobs=-1\n",
    ")\n",
    "\n",
    "#  Train XGBoost with Sample Weights\n",
    "print(\"\\n Training XGBoost (Handling Class Imbalance)...\")\n",
    "xgb_model.fit(X_train, y_train, sample_weight=np.array([class_weight_dict[i] for i in y_train]))\n",
    "print(\" XGBoost Training Completed!\")\n",
    "\n",
    "#  Define Base Models (RF + XGB + Logistic Regression)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=150, max_depth=25, class_weight=\"balanced\", random_state=42, n_jobs=-1)),\n",
    "    ('xgb', xgb_model),\n",
    "    ('logreg', LogisticRegression(class_weight=\"balanced\", max_iter=500, solver=\"saga\", multi_class=\"multinomial\", n_jobs=-1))\n",
    "]\n",
    "\n",
    "#  Define Meta-Model (LightGBM)\n",
    "meta_model = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, max_depth=7, \n",
    "                                num_leaves=40, random_state=42, \n",
    "                                is_unbalance=True,  # Helps with class imbalance\n",
    "                                min_gain_to_split=0.01,  # Prevents over-pruning\n",
    "                                device=\"gpu\" if gpu_available else \"cpu\")\n",
    "\n",
    "#  Define Stacking Classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "#  Train Stacking Model\n",
    "print(\"\\n Training Optimized Stacking Classifier...\")\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\" Optimized Stacking Classifier Training Completed in {end_time - start_time:.2f} seconds!\")\n",
    "\n",
    "#  Predictions\n",
    "print(\"\\n Making predictions...\")\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)\n",
    "\n",
    "#  Evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1-score\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    }\n",
    "    print(f\"\\n {model_name} Results:\")\n",
    "    print(results)\n",
    "    print(f\"\\n Confusion Matrix ({model_name}):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "#  Evaluate Stacked Model\n",
    "print(\"\\n Evaluating Optimized Stacking Classifier...\")\n",
    "evaluate_model(y_test, y_pred, y_pred_proba, \"Stacking Classifier\")\n",
    "\n",
    "print(\"\\n Training & Evaluation Complete! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Optuna Params all columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#  Check GPU Availability\n",
    "gpu_available = xgb.XGBClassifier().get_params().get('device', 'cpu') == 'cuda'\n",
    "print(f\" GPU Available: {gpu_available}\")\n",
    "\n",
    "#  Load dataset\n",
    "print(\"\\n Loading dataset...\")\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "print(f\" Dataset Loaded! Shape: {df.shape}\")\n",
    "\n",
    "#  Encode categorical labels\n",
    "print(\"\\n Encoding categorical labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Class\"] = label_encoder.fit_transform(df[\"Class\"])\n",
    "df[\"theft\"] = label_encoder.fit_transform(df[\"theft\"])\n",
    "print(\" Encoding complete!\")\n",
    "\n",
    "#  Feature selection\n",
    "feature_cols = [\n",
    "    \"Electricity:Facility [kW](Hourly)\", \"Fans:Electricity [kW](Hourly)\", \"Cooling:Electricity [kW](Hourly)\",\n",
    "    \"Heating:Electricity [kW](Hourly)\", \"InteriorLights:Electricity [kW](Hourly)\", \"InteriorEquipment:Electricity [kW](Hourly)\",\n",
    "    \"Gas:Facility [kW](Hourly)\", \"Heating:Gas [kW](Hourly)\", \"InteriorEquipment:Gas [kW](Hourly)\",\n",
    "    \"Water Heater:WaterSystems:Gas [kW](Hourly)\"\n",
    "]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"theft\"].values\n",
    "n_classes = len(np.unique(y))  # Number of unique classes\n",
    "\n",
    "#  Compute Class Weights for Imbalance Handling\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in np.unique(y)}\n",
    "\n",
    "#  Split dataset\n",
    "print(\"\\n Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(f\" Split complete! Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "#  Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#  Define XGBoost Model (Updated Parameters)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.0997, max_depth=11, min_child_weight=4, subsample=0.8696,\n",
    "    colsample_bytree=0.6797, n_estimators=126, \n",
    "    tree_method=\"gpu_hist\" if gpu_available else \"hist\",\n",
    "    objective=\"multi:softprob\", num_class=n_classes,\n",
    "    device=\"cuda\" if gpu_available else \"cpu\", n_jobs=-1\n",
    ")\n",
    "\n",
    "#  Train XGBoost with Sample Weights\n",
    "print(\"\\n Training XGBoost (Handling Class Imbalance)...\")\n",
    "xgb_model.fit(X_train, y_train, sample_weight=np.array([class_weight_dict[i] for i in y_train]))\n",
    "print(\" XGBoost Training Completed!\")\n",
    "\n",
    "#  Define Base Models (RF + XGB + Logistic Regression)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=184, max_depth=11, min_samples_split=18, min_samples_leaf=4, max_features='sqrt',\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    )),\n",
    "    ('xgb', xgb_model),\n",
    "    ('logreg', LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=1000, C=13812.3,\n",
    "        class_weight=\"balanced\", multi_class=\"multinomial\", n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "#  Define Meta-Model (Updated LightGBM Parameters)\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    learning_rate=0.0987, num_leaves=190, max_depth=11, min_data_in_leaf=12,\n",
    "    max_bin=356, feature_fraction=0.546, random_state=42,\n",
    "    is_unbalance=True,  # Helps with class imbalance\n",
    "    min_gain_to_split=0.01,  # Prevents over-pruning\n",
    "    device=\"gpu\" if gpu_available else \"cpu\"\n",
    ")\n",
    "\n",
    "#  Define Stacking Classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "#  Train Stacking Model\n",
    "print(\"\\n Training Optimized Stacking Classifier...\")\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\" Optimized Stacking Classifier Training Completed in {end_time - start_time:.2f} seconds!\")\n",
    "\n",
    "#  Predictions\n",
    "print(\"\\n Making predictions...\")\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)\n",
    "\n",
    "#  Evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1-score\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    }\n",
    "    print(f\"\\nüîç {model_name} Results:\")\n",
    "    print(results)\n",
    "    print(f\"\\n Confusion Matrix ({model_name}):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "#  Evaluate Stacked Model\n",
    "print(\"\\n Evaluating Optimized Stacking Classifier...\")\n",
    "evaluate_model(y_test, y_pred, y_pred_proba, \"Stacking Classifier\")\n",
    "\n",
    "print(\"\\n Training & Evaluation Complete! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Optuna Params without gas columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#  Check GPU Availability\n",
    "gpu_available = xgb.XGBClassifier().get_params().get('device', 'cpu') == 'cuda'\n",
    "print(f\" GPU Available: {gpu_available}\")\n",
    "\n",
    "#  Load dataset\n",
    "print(\"\\n Loading dataset...\")\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "print(f\" Dataset Loaded! Shape: {df.shape}\")\n",
    "\n",
    "#  Drop columns related to gas usage\n",
    "gas_columns = [\n",
    "    \"Gas:Facility [kW](Hourly)\",\n",
    "    \"Heating:Gas [kW](Hourly)\",\n",
    "    \"InteriorEquipment:Gas [kW](Hourly)\",\n",
    "    \"Water Heater:WaterSystems:Gas [kW](Hourly)\"\n",
    "]\n",
    "df = df.drop(columns=gas_columns, errors='ignore')\n",
    "\n",
    "#  Encode categorical labels\n",
    "print(\"\\n Encoding categorical labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Class\"] = label_encoder.fit_transform(df[\"Class\"])\n",
    "df[\"theft\"] = label_encoder.fit_transform(df[\"theft\"])\n",
    "print(\" Encoding complete!\")\n",
    "\n",
    "#  Separate features and target\n",
    "X = df.drop(columns=[\"theft\"])\n",
    "y = df[\"theft\"]\n",
    "n_classes = len(np.unique(y))  # Number of unique classes\n",
    "\n",
    "#  Compute Class Weights for Imbalance Handling\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in np.unique(y)}\n",
    "\n",
    "#  Split dataset\n",
    "print(\"\\n Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(f\" Split complete! Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "#  Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#  Define XGBoost Model (Handles Class Weights)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.09144751092423227,\n",
    "    max_depth=12,\n",
    "    min_child_weight=7,\n",
    "    subsample=0.8781111961629534,\n",
    "    colsample_bytree=0.8584849992349836,\n",
    "    n_estimators=200,\n",
    "    tree_method=\"gpu_hist\" if gpu_available else \"hist\",\n",
    "    objective=\"multi:softprob\", num_class=n_classes,\n",
    "    device=\"cuda\" if gpu_available else \"cpu\", n_jobs=-1\n",
    ")\n",
    "\n",
    "#  Train XGBoost with Sample Weights\n",
    "print(\"\\n Training XGBoost (Handling Class Imbalance)...\")\n",
    "xgb_model.fit(X_train, y_train, sample_weight=np.array([class_weight_dict[i] for i in y_train]))\n",
    "print(\" XGBoost Training Completed!\")\n",
    "\n",
    "#  Define Base Models (RF + XGB + Logistic Regression)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=136,\n",
    "        max_depth=12,\n",
    "        min_samples_split=7,\n",
    "        min_samples_leaf=17,\n",
    "        max_features='sqrt',\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('xgb', xgb_model),\n",
    "    ('logreg', LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=506,\n",
    "        C=11344.3650513728,\n",
    "        class_weight=\"balanced\",\n",
    "        multi_class=\"multinomial\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "#  Define Meta-Model (LightGBM)\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    learning_rate=0.02893527514545242,\n",
    "    num_leaves=299,\n",
    "    max_depth=11,\n",
    "    min_data_in_leaf=15,\n",
    "    max_bin=393,\n",
    "    feature_fraction=0.7447012597505922,\n",
    "    random_state=42,\n",
    "    is_unbalance=True,\n",
    "    min_gain_to_split=0.01,\n",
    "    device=\"gpu\" if gpu_available else \"cpu\"\n",
    ")\n",
    "\n",
    "#  Define Stacking Classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "#  Train Stacking Model\n",
    "print(\"\\n Training Optimized Stacking Classifier...\")\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\" Optimized Stacking Classifier Training Completed in {end_time - start_time:.2f} seconds!\")\n",
    "\n",
    "#  Predictions\n",
    "print(\"\\n Making predictions...\")\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)\n",
    "\n",
    "#  Evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1-score\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    }\n",
    "    print(f\"\\n {model_name} Results:\")\n",
    "    print(results)\n",
    "    print(f\"\\n Confusion Matrix ({model_name}):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "#  Evaluate Stacked Model\n",
    "print(\"\\n Evaluating Optimized Stacking Classifier...\")\n",
    "evaluate_model(y_test, y_pred, y_pred_proba, \"Stacking Classifier\")\n",
    "\n",
    "print(\"\\n Training & Evaluation Complete! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna + ExtraaTrees + all columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#  Check GPU Availability\n",
    "gpu_available = xgb.XGBClassifier().get_params().get('device', 'cpu') == 'cuda'\n",
    "print(f\" GPU Available: {gpu_available}\")\n",
    "\n",
    "#  Load dataset\n",
    "print(\"\\n Loading dataset...\")\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "print(f\" Dataset Loaded! Shape: {df.shape}\")\n",
    "\n",
    "#  Encode categorical labels\n",
    "print(\"\\n Encoding categorical labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Class\"] = label_encoder.fit_transform(df[\"Class\"])\n",
    "df[\"theft\"] = label_encoder.fit_transform(df[\"theft\"])\n",
    "print(\" Encoding complete!\")\n",
    "\n",
    "#  Feature selection\n",
    "feature_cols = [\n",
    "    \"Electricity:Facility [kW](Hourly)\", \"Fans:Electricity [kW](Hourly)\", \"Cooling:Electricity [kW](Hourly)\",\n",
    "    \"Heating:Electricity [kW](Hourly)\", \"InteriorLights:Electricity [kW](Hourly)\", \"InteriorEquipment:Electricity [kW](Hourly)\",\n",
    "    \"Gas:Facility [kW](Hourly)\", \"Heating:Gas [kW](Hourly)\", \"InteriorEquipment:Gas [kW](Hourly)\",\n",
    "    \"Water Heater:WaterSystems:Gas [kW](Hourly)\"\n",
    "]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[\"theft\"].values\n",
    "n_classes = len(np.unique(y))  # Number of unique classes\n",
    "\n",
    "#  Compute Class Weights for Imbalance Handling\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in np.unique(y)}\n",
    "\n",
    "#  Split dataset\n",
    "print(\"\\n Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(f\" Split complete! Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "#  Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#  Define XGBoost Model (Updated Parameters)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.0997, max_depth=11, min_child_weight=4, subsample=0.8696,\n",
    "    colsample_bytree=0.6797, n_estimators=126, \n",
    "    tree_method=\"gpu_hist\" if gpu_available else \"hist\",\n",
    "    objective=\"multi:softprob\", num_class=n_classes,\n",
    "    device=\"cuda\" if gpu_available else \"cpu\", n_jobs=-1\n",
    ")\n",
    "\n",
    "#  Train XGBoost with Sample Weights\n",
    "print(\"\\n Training XGBoost (Handling Class Imbalance)...\")\n",
    "xgb_model.fit(X_train, y_train, sample_weight=np.array([class_weight_dict[i] for i in y_train]))\n",
    "print(\" XGBoost Training Completed!\")\n",
    "\n",
    "#  Define Base Models (RF + XGB + Extra Trees)\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=184, max_depth=11, min_samples_split=18, min_samples_leaf=4, max_features='sqrt',\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    )),\n",
    "    ('xgb', xgb_model),\n",
    "    ('et', ExtraTreesClassifier(\n",
    "        n_estimators=121, max_depth=12, min_samples_split=3, min_samples_leaf=9, max_features=None,\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "\n",
    "#  Define Meta-Model (Updated LightGBM Parameters)\n",
    "meta_model = lgb.LGBMClassifier(\n",
    "    boosting_type=\"gbdt\",\n",
    "    learning_rate=0.0987, num_leaves=190, max_depth=11, min_data_in_leaf=12,\n",
    "    max_bin=356, feature_fraction=0.546, random_state=42,\n",
    "    is_unbalance=True,  # Helps with class imbalance\n",
    "    min_gain_to_split=0.01,  # Prevents over-pruning\n",
    "    device=\"gpu\" if gpu_available else \"cpu\"\n",
    ")\n",
    "\n",
    "#  Define Stacking Classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1)\n",
    "\n",
    "#  Train Stacking Model\n",
    "print(\"\\n Training Optimized Stacking Classifier...\")\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\" Optimized Stacking Classifier Training Completed in {end_time - start_time:.2f} seconds!\")\n",
    "\n",
    "#  Predictions\n",
    "print(\"\\n Making predictions...\")\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)\n",
    "\n",
    "#  Evaluation function\n",
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1-score\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba, multi_class=\"ovr\")\n",
    "    }\n",
    "    print(f\"\\nüîç {model_name} Results:\")\n",
    "    print(results)\n",
    "    print(f\"\\n Confusion Matrix ({model_name}):\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "#  Evaluate Stacked Model\n",
    "print(\"\\n Evaluating Optimized Stacking Classifier...\")\n",
    "evaluate_model(y_test, y_pred, y_pred_proba, \"Stacking Classifier\")\n",
    "\n",
    "print(\"\\n Training & Evaluation Complete! \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEE213-IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
