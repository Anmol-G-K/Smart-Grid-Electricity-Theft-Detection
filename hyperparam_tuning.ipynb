{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna with all columns\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "X = df.drop(['theft', 'Class'], axis=1)  # Drop target and non-numeric columns\n",
    "y = df['theft']\n",
    "\n",
    "# Encode the target variable for multiclass classification\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define an objective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['lgbm', 'xgboost', 'logreg', 'rf'])\n",
    "\n",
    "    # Define hyperparameters for each model\n",
    "    if model_name == 'lgbm':\n",
    "        param = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'max_bin': trial.suggest_int('max_bin', 100, 500),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "            'class_weight': class_weight_dict\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "\n",
    "    elif model_name == 'xgboost':\n",
    "        param = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'scale_pos_weight': class_weights[1]  # Use for handling imbalance in xgboost\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "\n",
    "    elif model_name == 'logreg':\n",
    "        param = {\n",
    "            'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "            'C': trial.suggest_loguniform('C', 1e-5, 1e5),\n",
    "            'multi_class': 'ovr',\n",
    "            'class_weight': 'balanced'  # Use balanced class weights for Logistic Regression\n",
    "        }\n",
    "        model = LogisticRegression(**param)\n",
    "\n",
    "    elif model_name == 'rf':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),  # Fix: 'auto' replaced with 'sqrt' or 'log2'\n",
    "            'class_weight': 'balanced'  # Use balanced class weights for RandomForest\n",
    "        }\n",
    "        model = RandomForestClassifier(**param)\n",
    "\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Set up the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial}\")\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Gas columns\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "# Drop the columns related to gas\n",
    "df = df.drop(columns=['Gas:Facility [kW](Hourly)', 'Heating:Gas [kW](Hourly)',\n",
    "                      'InteriorEquipment:Gas [kW](Hourly)', 'Water Heater:WaterSystems:Gas [kW](Hourly)'])\n",
    "\n",
    "# Sanitize column names to avoid characters like '[' or ']'\n",
    "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Prepare your dataset (replace 'your_dataset.csv' with actual file)\n",
    "X = df.drop(['theft', 'Class'], axis=1)  # Drop target and non-numeric columns\n",
    "y = df['theft']\n",
    "\n",
    "# Encode the target variable for multiclass classification\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define an objective function for Optuna\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical('model', ['lgbm', 'xgboost', 'logreg', 'rf'])\n",
    "\n",
    "    # Define hyperparameters for each model\n",
    "    if model_name == 'lgbm':\n",
    "        param = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'max_bin': trial.suggest_int('max_bin', 100, 500),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "            'class_weight': class_weight_dict\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**param)\n",
    "\n",
    "    elif model_name == 'xgboost':\n",
    "        param = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'scale_pos_weight': class_weights[1]  # Use for handling imbalance in xgboost\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "\n",
    "    elif model_name == 'logreg':\n",
    "        param = {\n",
    "            'solver': trial.suggest_categorical('solver', ['lbfgs', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "            'C': trial.suggest_loguniform('C', 1e-5, 1e5),\n",
    "            'multi_class': 'ovr',\n",
    "            'class_weight': 'balanced'  # Use balanced class weights for Logistic Regression\n",
    "        }\n",
    "        model = LogisticRegression(**param)\n",
    "\n",
    "    elif model_name == 'rf':\n",
    "        param = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),  # Fix: 'auto' replaced with 'sqrt' or 'log2'\n",
    "            'class_weight': 'balanced'  # Use balanced class weights for RandomForest\n",
    "        }\n",
    "        model = RandomForestClassifier(**param)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Set up the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial}\")\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEE213-IML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
